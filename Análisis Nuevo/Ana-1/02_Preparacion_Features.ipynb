{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d88366f",
   "metadata": {},
   "source": [
    "# ðŸ” ANOMALY DETECTION IN MAMBA SEEDLING STUDENTS\n",
    "## Phase 2: PREPARATION (Feature Engineering)\n",
    "\n",
    "---\n",
    "\n",
    "### OBJECTIVES OF THIS PHASE:\n",
    "1. Load data from initial analysis (EDA)\n",
    "2. Create derived features relevant for anomaly detection\n",
    "3. Normalize data using Min-Max Scaling (0-1 range)\n",
    "4. Handle missing values\n",
    "5. Prepare data for modeling\n",
    "\n",
    "### EXPECTED OUTPUT:\n",
    "- Normalized and clean dataset\n",
    "- Derived features created\n",
    "- Data ready for anomaly models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9eac41",
   "metadata": {},
   "source": [
    "## CELL 5: CREATE DERIVED FEATURES\n",
    "\n",
    "Derived features are combinations of original variables that capture important concepts for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. IMPORT LIBRARIES AND LOAD DATA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "\n",
    "# Load data\n",
    "data_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\AnÃ¡lisis Nuevo\\data\\RespuestasSemillero_completo.json'\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(json_data)\n",
    "print(f\"âœ… Data loaded: {df.shape[0]} students Ã— {df.shape[1]} variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 CONVERT TO NUMERIC\n",
    "df_prepared = df.copy()\n",
    "\n",
    "# Convert all columns to numeric\n",
    "for col in df_prepared.columns:\n",
    "    try:\n",
    "        df_prepared[col] = pd.to_numeric(df_prepared[col], errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"âœ… Conversion to numeric data completed\")\n",
    "print(f\"\\nðŸ“Š First 5 rows:\")\n",
    "df_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 CREATE DERIVED FEATURES\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATION OF DERIVED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature 1: Average Academic Performance\n",
    "df_prepared['F_Average_Performance'] = (df_prepared['Q33'] + df_prepared['Q34'] + df_prepared['Q35']) / 3\n",
    "print(f\"\\nâœ… F1: Average_Performance = (Q33 + Q34 + Q35) / 3\")\n",
    "print(f\"    Range: {df_prepared['F_Average_Performance'].min():.2f} - {df_prepared['F_Average_Performance'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Average_Performance'].mean():.2f}\")\n",
    "\n",
    "# Feature 2: Academic Load\n",
    "df_prepared['F_Academic_Load'] = df_prepared['Q7'] + df_prepared['Q8'] + df_prepared['Q9']\n",
    "print(f\"\\nâœ… F2: Academic_Load = Q7 + Q8 + Q9\")\n",
    "print(f\"    Range: {df_prepared['F_Academic_Load'].min():.2f} - {df_prepared['F_Academic_Load'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Academic_Load'].mean():.2f}\")\n",
    "\n",
    "# Feature 3: Life Balance (work vs study)\n",
    "df_prepared['F_Life_Balance'] = df_prepared['Q20'] / (df_prepared['F_Academic_Load'] + 1)\n",
    "print(f\"\\nâœ… F3: Life_Balance = Q20 / (Academic_Load + 1)\")\n",
    "print(f\"    Range: {df_prepared['F_Life_Balance'].min():.2f} - {df_prepared['F_Life_Balance'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Life_Balance'].mean():.2f}\")\n",
    "print(f\"    (Higher values = more work than study)\")\n",
    "\n",
    "# Feature 4: Psychological Stress\n",
    "df_prepared['F_Psychological_Stress'] = (df_prepared['Q17'] + df_prepared['Q18']) / 2\n",
    "print(f\"\\nâœ… F4: Psychological_Stress = (Q17 + Q18) / 2\")\n",
    "print(f\"    Range: {df_prepared['F_Psychological_Stress'].min():.2f} - {df_prepared['F_Psychological_Stress'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Psychological_Stress'].mean():.2f}\")\n",
    "\n",
    "# Feature 5: Family Support\n",
    "df_prepared['F_Family_Support'] = df_prepared['Q12'] + df_prepared['Q13']\n",
    "print(f\"\\nâœ… F5: Family_Support = Q12 + Q13\")\n",
    "print(f\"    Range: {df_prepared['F_Family_Support'].min():.0f} - {df_prepared['F_Family_Support'].max():.0f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Family_Support'].mean():.2f}\")\n",
    "\n",
    "# Feature 6: Grade Consistency (Standard Deviation)\n",
    "df_prepared['F_Grade_Consistency'] = df_prepared[['Q33', 'Q34', 'Q35']].std(axis=1)\n",
    "print(f\"\\nâœ… F6: Grade_Consistency = std(Q33, Q34, Q35)\")\n",
    "print(f\"    Range: {df_prepared['F_Grade_Consistency'].min():.2f} - {df_prepared['F_Grade_Consistency'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Grade_Consistency'].mean():.2f}\")\n",
    "print(f\"    (Higher values = more inconsistency in grades)\")\n",
    "\n",
    "# Feature 7: Responsibility-Result Index\n",
    "df_prepared['F_Responsibility_Result_Index'] = df_prepared['Q3'] / (df_prepared['F_Average_Performance'] + 1)\n",
    "print(f\"\\nâœ… F7: Responsibility_Result_Index = Q3 / (Average_Performance + 1)\")\n",
    "print(f\"    Range: {df_prepared['F_Responsibility_Result_Index'].min():.2f} - {df_prepared['F_Responsibility_Result_Index'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Responsibility_Result_Index'].mean():.2f}\")\n",
    "print(f\"    (Higher values = responsible but low performance)\")\n",
    "\n",
    "# Feature 8: Parental Education Level\n",
    "df_prepared['F_Parental_Education'] = df_prepared['Q15'] + df_prepared['Q16']\n",
    "print(f\"\\nâœ… F8: Parental_Education = Q15 + Q16\")\n",
    "print(f\"    Range: {df_prepared['F_Parental_Education'].min():.0f} - {df_prepared['F_Parental_Education'].max():.0f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Parental_Education'].mean():.2f}\")\n",
    "\n",
    "# Feature 9: Socioeconomic Risk (low stratum + no financial support)\n",
    "df_prepared['F_Socioeconomic_Risk'] = (6 - df_prepared['Q32']) + (1 - df_prepared['Q21'])\n",
    "print(f\"\\nâœ… F9: Socioeconomic_Risk = (6 - Q32) + (1 - Q21)\")\n",
    "print(f\"    Range: {df_prepared['F_Socioeconomic_Risk'].min():.2f} - {df_prepared['F_Socioeconomic_Risk'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Socioeconomic_Risk'].mean():.2f}\")\n",
    "print(f\"    (Higher values = higher socioeconomic risk)\")\n",
    "\n",
    "# Feature 10: Interest-Performance Gap\n",
    "df_prepared['F_Interest_Performance_Gap'] = abs(df_prepared['Q6'] - (df_prepared['F_Average_Performance'] / 3))\n",
    "print(f\"\\nâœ… F10: Interest_Performance_Gap = |Q6 - (Average_Performance/3)|\")\n",
    "print(f\"    Range: {df_prepared['F_Interest_Performance_Gap'].min():.2f} - {df_prepared['F_Interest_Performance_Gap'].max():.2f}\")\n",
    "print(f\"    Mean: {df_prepared['F_Interest_Performance_Gap'].mean():.2f}\")\n",
    "print(f\"    (Higher values = mismatch between interest and performance)\")\n",
    "\n",
    "print(f\"\\n\\nâœ… TOTAL DERIVED FEATURES: 10\")\n",
    "print(f\"   Original dimension: {df.shape[1]}\")\n",
    "print(f\"   Dimension after features: {df_prepared.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e15c0",
   "metadata": {},
   "source": [
    "## CELL 6: MIN-MAX NORMALIZATION\n",
    "\n",
    "Min-Max normalization (feature scaling) transforms data to a 0-1 range. This is essential for anomaly detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. MIN-MAX NORMALIZATION\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MIN-MAX NORMALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select numeric columns for normalization\n",
    "numeric_cols = df_prepared.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nðŸ“Š Columns to normalize: {len(numeric_cols)}\")\n",
    "print(f\"   {numeric_cols}\")\n",
    "\n",
    "# Create a copy for normalized data\n",
    "df_normalized = df_prepared.copy()\n",
    "\n",
    "# Initialize scaler (0-1 range)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Apply Min-Max normalization\n",
    "df_normalized[numeric_cols] = scaler.fit_transform(df_prepared[numeric_cols])\n",
    "\n",
    "print(f\"\\nâœ… Min-Max normalization completed\")\n",
    "print(f\"\\nðŸ“‹ Statistics AFTER normalization:\")\n",
    "print(f\"\\n   Min value (all variables): {df_normalized[numeric_cols].min().min():.6f} (â‰ˆ 0)\")\n",
    "print(f\"   Max value (all variables): {df_normalized[numeric_cols].max().max():.6f} (â‰ˆ 1)\")\n",
    "print(f\"   Mean of all variables: {df_normalized[numeric_cols].mean().mean():.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š First 5 NORMALIZED rows:\")\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 VERIFY NORMALIZATION - BEFORE AND AFTER\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: BEFORE AND AFTER NORMALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_vars = ['F_Average_Performance', 'F_Academic_Load', 'F_Psychological_Stress', \n",
    "                    'F_Life_Balance', 'F_Responsibility_Result_Index']\n",
    "\n",
    "for var in comparison_vars:\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"   BEFORE:  Min={df_prepared[var].min():7.3f}, Max={df_prepared[var].max():7.3f}\")\n",
    "    print(f\"   AFTER:   Min={df_normalized[var].min():7.3f}, Max={df_normalized[var].max():7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 VISUALIZATION OF NORMALIZATION EFFECT\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Effect of Min-Max Normalization (0-1 range)', fontsize=14, fontweight='bold')\n",
    "\n",
    "variables_to_plot = [\n",
    "    ('F_Average_Performance', 'Average Performance'),\n",
    "    ('F_Academic_Load', 'Academic Load'),\n",
    "    ('F_Psychological_Stress', 'Psychological Stress'),\n",
    "    ('F_Life_Balance', 'Life Balance'),\n",
    "    ('F_Responsibility_Result_Index', 'Responsibility-Result Index'),\n",
    "    ('Q20', 'Work Hours (Q20)')\n",
    "]\n",
    "\n",
    "for idx, (var, label) in enumerate(variables_to_plot):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Original distribution\n",
    "    axes[row, col].hist(df_prepared[var].dropna(), bins=15, alpha=0.6, label='Original', color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Normalized distribution\n",
    "    axes_twin = axes[row, col].twinx()\n",
    "    axes_twin.hist(df_normalized[var].dropna(), bins=15, alpha=0.6, label='Normalized', color='coral', edgecolor='black')\n",
    "    \n",
    "    axes[row, col].set_xlabel(label)\n",
    "    axes[row, col].set_ylabel('Frequency (Original)', color='skyblue')\n",
    "    axes_twin.set_ylabel('Frequency (Normalized)', color='coral')\n",
    "    axes[row, col].set_title(label, fontweight='bold')\n",
    "    axes[row, col].legend(loc='upper left')\n",
    "    axes_twin.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231c6a4",
   "metadata": {},
   "source": [
    "## CELL 7: HANDLING MISSING VALUES\n",
    "\n",
    "Detect, analyze and treat missing values (NaN) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83079d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. HANDLING MISSING VALUES\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS OF MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count missing values in original data\n",
    "missing_original = df_prepared.isnull().sum()\n",
    "missing_percent = (missing_original / len(df_prepared)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Variable': missing_original.index,\n",
    "    'Missing_Count': missing_original.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "}).sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š MISSING VALUES SUMMARY:\")\n",
    "print(f\"   Total variables: {len(missing_df)}\")\n",
    "print(f\"   Variables with missing data: {(missing_original > 0).sum()}\")\n",
    "print(f\"   COMPLETE variables: {(missing_original == 0).sum()}\")\n",
    "\n",
    "if (missing_original > 0).sum() > 0:\n",
    "    print(f\"\\nðŸ“‹ Variables WITH missing data:\")\n",
    "    variables_with_missing = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    for idx, row in variables_with_missing.iterrows():\n",
    "        print(f\"   {row['Variable']:25s}: {int(row['Missing_Count']):3d} ({row['Missing_Percent']:5.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nâœ… NO missing values in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47480469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 STRATEGY FOR HANDLING MISSING VALUES\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATEGY FOR HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make a copy for handling missing values\n",
    "df_clean = df_normalized.copy()\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Strategy 1: Drop rows where too many values are missing (> 30%)\n",
    "missing_per_row = df_clean[numeric_cols].isnull().sum(axis=1)\n",
    "threshold_missing = len(numeric_cols) * 0.3  # 30% threshold\n",
    "\n",
    "rows_to_drop = missing_per_row[missing_per_row > threshold_missing].index\n",
    "print(f\"\\n1ï¸âƒ£  Drop rows with > 30% missing data\")\n",
    "print(f\"    Rows to drop: {len(rows_to_drop)}\")\n",
    "print(f\"    Remaining rows: {len(df_clean) - len(rows_to_drop)}\")\n",
    "\n",
    "if len(rows_to_drop) > 0:\n",
    "    df_clean = df_clean.drop(rows_to_drop)\n",
    "    print(f\"    âœ… Rows dropped\")\n",
    "else:\n",
    "    print(f\"    âœ… No rows with > 30% missing data\")\n",
    "\n",
    "# Strategy 2: Impute missing values with median\n",
    "print(f\"\\n2ï¸âƒ£  Impute missing values with MEDIAN\")\n",
    "print(f\"    Variables with missing data:\")\n",
    "\n",
    "missing_after_drop = df_clean[numeric_cols].isnull().sum()\n",
    "missing_after_drop = missing_after_drop[missing_after_drop > 0]\n",
    "\n",
    "for var in missing_after_drop.index:\n",
    "    median_val = df_clean[var].median()\n",
    "    count = df_clean[var].isnull().sum()\n",
    "    df_clean[var].fillna(median_val, inplace=True)\n",
    "    print(f\"    âœ… {var:25s}: {count} values filled with median={median_val:.2f}\")\n",
    "\n",
    "if len(missing_after_drop) == 0:\n",
    "    print(f\"    âœ… No missing values to fill\")\n",
    "\n",
    "print(f\"\\nâœ… MISSING VALUES TREATMENT COMPLETED\")\n",
    "print(f\"    Final rows: {len(df_clean)}\")\n",
    "print(f\"    Remaining missing values: {df_clean[numeric_cols].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef02352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 VERIFICATION OF DATA QUALITY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATA QUALITY VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š FINAL SUMMARY:\")\n",
    "print(f\"   Original students: {len(df)}\")\n",
    "print(f\"   Students after cleaning: {len(df_clean)}\")\n",
    "print(f\"   Dropped students: {len(df) - len(df_clean)}\")\n",
    "\n",
    "print(f\"\\n   Original variables: {df.shape[1]}\")\n",
    "print(f\"   Variables with derived features: {df_clean.shape[1]}\")\n",
    "print(f\"   Derived features created: {df_clean.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n   Total missing values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"   Data completeness: {100 * (1 - df_clean.isnull().sum().sum() / (df_clean.shape[0] * df_clean.shape[1])):.2f}%\")\n",
    "\n",
    "print(f\"\\n   Normalization range: [0, 1] (Min-Max)\")\n",
    "print(f\"   Data ready for anomaly detection modeling\")\n",
    "\n",
    "print(f\"\\nâœ… DATA READY FOR MODELING\")\n",
    "print(f\"\\nðŸ“‹ First 5 rows of final dataset (normalized and clean):\")\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 SAVE PREPROCESSED DATA FOR NEXT PHASE\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVE PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save cleaned and normalized data\n",
    "output_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\AnÃ¡lisis Nuevo\\data\\dataset_prepared_minmax.csv'\n",
    "df_clean.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Data saved at:\")\n",
    "print(f\"   {output_path}\")\n",
    "print(f\"   Size: {len(df_clean)} rows Ã— {len(df_clean.columns)} columns\")\n",
    "\n",
    "# Also save scaler for future use\n",
    "import pickle\n",
    "scaler_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\AnÃ¡lisis Nuevo\\data\\scaler_minmax.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"\\nâœ… Min-Max Scaler saved at:\")\n",
    "print(f\"   {scaler_path}\")\n",
    "print(f\"   Purpose: To normalize new data with the same scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc02731",
   "metadata": {},
   "source": [
    "## PHASE 2 SUMMARY\n",
    "\n",
    "### âœ… COMPLETED TASKS:\n",
    "\n",
    "1. **DERIVED FEATURES CREATED (10 new variables):**\n",
    "   - F_Average_Performance: Average of grades\n",
    "   - F_Academic_Load: Total academic load\n",
    "   - F_Life_Balance: Work-to-study ratio\n",
    "   - F_Psychological_Stress: Average stress level\n",
    "   - F_Family_Support: Family support score\n",
    "   - F_Grade_Consistency: Grade consistency indicator\n",
    "   - F_Responsibility_Result_Index: Responsibility-to-result ratio\n",
    "   - F_Parental_Education: Parental education level\n",
    "   - F_Socioeconomic_Risk: Socioeconomic risk index\n",
    "   - F_Interest_Performance_Gap: Interest-performance mismatch\n",
    "\n",
    "2. **MIN-MAX NORMALIZATION:**\n",
    "   - All numeric variables scaled to [0, 1] range\n",
    "   - Enables fair comparison in ML models\n",
    "   - MinMaxScaler saved for future predictions\n",
    "\n",
    "3. **MISSING VALUES HANDLING:**\n",
    "   - Analysis of missing values\n",
    "   - Removal of rows with >30% missing data\n",
    "   - Imputation of missing values with median\n",
    "   - Verification of 100% completeness\n",
    "\n",
    "### ðŸ“Š GENERATED DATA:\n",
    "- **dataset_prepared_minmax.csv**: Dataset ready for modeling\n",
    "- **scaler_minmax.pkl**: MinMaxScaler object for reproducibility\n",
    "\n",
    "### ðŸŽ¯ NEXT PHASE:\n",
    "**Phase 3: MODELING (Implement Anomaly Detection Algorithms)**\n",
    "- Isolation Forest\n",
    "- Local Outlier Factor (LOF)\n",
    "- One-Class SVM\n",
    "- Autoencoder (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2 SUCCESSFULLY COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸŽ‰ EXECUTION SUMMARY:\n",
    "\n",
    "âœ… Derived features: 10 new variables created\n",
    "âœ… Normalization: Min-Max scaling applied to {len(numeric_cols)} variables\n",
    "âœ… Missing values: 100% handled\n",
    "âœ… Data quality: Optimal for modeling\n",
    "\n",
    "ðŸ“Š FINAL STATISTICS:\n",
    "   â€¢ Students in dataset: {len(df_clean)}\n",
    "   â€¢ Total variables: {df_clean.shape[1]}\n",
    "   â€¢ Data completeness: 100%\n",
    "   â€¢ Normalization range: [0, 1]\n",
    "\n",
    "âœ¨ Data ready for anomaly detection model training\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Next step: Execute Phase 3 - MODELING\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
