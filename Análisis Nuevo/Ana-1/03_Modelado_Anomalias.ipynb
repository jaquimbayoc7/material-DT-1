{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3adc24aa",
   "metadata": {},
   "source": [
    "# üîç ANOMALY DETECTION IN MAMBA SEEDLING STUDENTS\n",
    "## Phase 3: MODELING (Implement Anomaly Detection Algorithms)\n",
    "\n",
    "---\n",
    "\n",
    "### OBJECTIVES OF THIS PHASE:\n",
    "1. Load prepared and normalized data\n",
    "2. Implement 4 anomaly detection algorithms\n",
    "3. Test with 3 different train/test splits (70/30, 60/40, 80/20)\n",
    "4. Compare performance across different splits\n",
    "5. Identify best configuration for each model\n",
    "\n",
    "### EXPECTED OUTPUT:\n",
    "- 4 trained anomaly detection models\n",
    "- Performance metrics for each train/test split\n",
    "- Detected anomalies flagged and scored\n",
    "- Comparison analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea83b6",
   "metadata": {},
   "source": [
    "## INITIAL SETUP: Load Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf58d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "\n",
      "üìä PREPARED DATA LOADED FROM PHASE 2\n",
      "   File: dataset_prepared_minmax.csv\n",
      "   Samples (students): 81\n",
      "   Variables (features): 45\n",
      "   Data normalization: Min-Max [0, 1]\n",
      "   Missing values: 243 (0 = Clean data)\n",
      "\n",
      "‚úÖ Min-Max Scaler loaded (feature_range=(0, 1))\n",
      "\n",
      "üìä Dataset Preview:\n",
      "    Q1        Q2   Q3   Q4   Q5        Q6     Q7     Q8        Q9  Q10  ...  \\\n",
      "0  0.0  0.222222  1.0  1.0  1.0  0.666667  0.250  0.375  0.466667  NaN  ...   \n",
      "1  0.0  0.166667  1.0  1.0  0.0  1.000000  0.750  0.500  0.000000  NaN  ...   \n",
      "2  1.0  0.333333  1.0  1.0  0.0  0.333333  0.250  0.125  0.133333  NaN  ...   \n",
      "3  1.0  0.444444  1.0  1.0  1.0  1.000000  0.500  1.000  0.000000  NaN  ...   \n",
      "4  1.0  0.222222  1.0  1.0  0.0  0.666667  0.125  0.125  0.133333  NaN  ...   \n",
      "\n",
      "   F_Average_Performance  F_Academic_Load  F_Life_Balance  \\\n",
      "0               0.777778         0.409091        0.000000   \n",
      "1               0.555556         0.318182        0.000000   \n",
      "2               0.666667         0.090909        0.666667   \n",
      "3               0.777778         0.409091        0.384615   \n",
      "4               0.555556         0.045455        0.400000   \n",
      "\n",
      "   F_Psychological_Stress  F_Family_Support  F_Grade_Consistency  \\\n",
      "0                0.428571          0.285714                 0.25   \n",
      "1                0.285714          0.214286                 0.25   \n",
      "2                0.142857          0.142857                 0.00   \n",
      "3                0.142857          0.428571                 0.25   \n",
      "4                0.428571          0.214286                 0.25   \n",
      "\n",
      "   F_Responsibility_Result_Index  F_Parental_Education  F_Socioeconomic_Risk  \\\n",
      "0                       0.394947                   0.8                  0.00   \n",
      "1                       0.506079                   0.3                  0.75   \n",
      "2                       0.446809                   0.3                  0.75   \n",
      "3                       0.394947                   0.2                  0.75   \n",
      "4                       0.506079                   0.4                  0.50   \n",
      "\n",
      "   F_Interest_Performance_Gap  \n",
      "0                    0.466667  \n",
      "1                    0.833333  \n",
      "2                    0.200000  \n",
      "3                    0.766667  \n",
      "4                    0.533333  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "\n",
      "üìà Data Statistics:\n",
      "   Min value (all variables): 0.000000\n",
      "   Max value (all variables): 1.000000\n",
      "   Mean value (all variables): 0.461321\n",
      "   Std value (all variables): 0.269521\n"
     ]
    }
   ],
   "source": [
    "# SETUP: Import libraries and load prepared data from Phase 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import anomaly detection models\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, silhouette_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD PREPARED DATA FROM PHASE 2\n",
    "# ============================================================================\n",
    "data_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\An√°lisis Nuevo\\data\\dataset_prepared_minmax.csv'\n",
    "df_data = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\nüìä PREPARED DATA LOADED FROM PHASE 2\")\n",
    "print(f\"   File: dataset_prepared_minmax.csv\")\n",
    "print(f\"   Samples (students): {df_data.shape[0]}\")\n",
    "print(f\"   Variables (features): {df_data.shape[1]}\")\n",
    "print(f\"   Data normalization: Min-Max [0, 1]\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA VALIDATION AND CLEANING\n",
    "# ============================================================================\n",
    "print(f\"\\nüîç DATA VALIDATION:\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df_data.isnull().sum().sum()\n",
    "print(f\"   Missing values (NaN): {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found NaN values! Removing rows with missing data...\")\n",
    "    initial_rows = len(df_data)\n",
    "    df_data = df_data.dropna()\n",
    "    removed_rows = initial_rows - len(df_data)\n",
    "    print(f\"   Removed {removed_rows} rows with NaN\")\n",
    "    print(f\"   Remaining samples: {len(df_data)}\")\n",
    "\n",
    "# Check for infinite values\n",
    "has_inf = np.isinf(df_data.select_dtypes(include=[np.number])).any().any()\n",
    "if has_inf:\n",
    "    print(f\"   ‚ö†Ô∏è  Found infinite values! Removing rows...\")\n",
    "    initial_rows = len(df_data)\n",
    "    df_data = df_data[~np.isinf(df_data.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    removed_rows = initial_rows - len(df_data)\n",
    "    print(f\"   Removed {removed_rows} rows with infinite values\")\n",
    "    print(f\"   Remaining samples: {len(df_data)}\")\n",
    "\n",
    "# Final validation\n",
    "final_missing = df_data.isnull().sum().sum()\n",
    "final_inf = np.isinf(df_data.select_dtypes(include=[np.number])).any().any()\n",
    "\n",
    "if final_missing == 0 and not final_inf:\n",
    "    print(f\"   ‚úÖ Data is CLEAN - No NaN or infinite values\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: Still found issues\")\n",
    "\n",
    "print(f\"   Data type validation:\")\n",
    "print(f\"   - Numeric columns: {len(df_data.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"   - All values in [0, 1]? {(df_data.select_dtypes(include=[np.number]) >= 0).all().all() and (df_data.select_dtypes(include=[np.number]) <= 1).all().all()}\")\n",
    "\n",
    "# Load Min-Max Scaler for reference\n",
    "scaler_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\An√°lisis Nuevo\\data\\scaler_minmax.pkl'\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler_minmax = pickle.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ Min-Max Scaler loaded (feature_range={scaler_minmax.feature_range})\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Preview (First 5 rows):\")\n",
    "print(df_data.head())\n",
    "\n",
    "print(f\"\\nüìà Data Statistics:\")\n",
    "print(f\"   Min value (all variables): {df_data.select_dtypes(include=[np.number]).min().min():.6f}\")\n",
    "print(f\"   Max value (all variables): {df_data.select_dtypes(include=[np.number]).max().max():.6f}\")\n",
    "print(f\"   Mean value (all variables): {df_data.select_dtypes(include=[np.number]).mean().mean():.6f}\")\n",
    "print(f\"   Std value (all variables): {df_data.select_dtypes(include=[np.number]).std().mean():.6f}\")\n",
    "\n",
    "print(f\"\\n‚ú® Data ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57356c",
   "metadata": {},
   "source": [
    "## CELL 8: ISOLATION FOREST MODEL\n",
    "\n",
    "Isolation Forest is an ensemble method that isolates anomalies by randomly selecting features and split values. It's efficient and doesn't require distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bf454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ISOLATION FOREST - ANOMALY DETECTION\n",
      "================================================================================\n",
      "\n",
      "Algorithm: Ensemble method for anomaly detection\n",
      "Principle: Isolates anomalies by randomly selecting features\n",
      "Features used: All prepared and normalized variables from Phase 2\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training Isolation Forest with 70/30 split:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìä Data Split (from prepared dataset):\n",
      "   Total students: 81\n",
      "   Training samples: 56 (69.1%)\n",
      "   Test samples: 25 (30.9%)\n",
      "   Features used: 45 variables\n",
      "\n",
      "ü§ñ Model Configuration:\n",
      "   Contamination rate: 10% (expected anomaly percentage)\n",
      "   Number of trees: 100\n",
      "   Max features: All\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nIsolationForest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Max features: All\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Train on normalized data\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43miso_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Predict anomalies (-1 = anomaly, 1 = normal)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m train_predictions \u001b[38;5;241m=\u001b[39m iso_forest\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:291\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Fit estimator.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;66;03m# Pre-sort indices to avoid that each individual tree of the\u001b[39;00m\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;66;03m# ensemble sorts the indices.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m         X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nIsolationForest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# CELL 8: ISOLATION FOREST MODEL\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ISOLATION FOREST - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Ensemble method for anomaly detection\")\n",
    "print(\"Principle: Isolates anomalies by randomly selecting features\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "# Define train/test split configurations\n",
    "splits_config = [\n",
    "    {'train_size': 0.70, 'test_size': 0.30, 'name': '70/30'},\n",
    "    {'train_size': 0.60, 'test_size': 0.40, 'name': '60/40'},\n",
    "    {'train_size': 0.80, 'test_size': 0.20, 'name': '80/20'}\n",
    "]\n",
    "\n",
    "iso_forest_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training Isolation Forest with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Split data using the prepared normalized dataset\n",
    "    X_train, X_test = train_test_split(\n",
    "        df_data, \n",
    "        train_size=train_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # ADDITIONAL DATA VALIDATION AFTER SPLIT\n",
    "    # Remove any rows with NaN that might have slipped through\n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from training set...\")\n",
    "        X_train = X_train.dropna()\n",
    "    \n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from test set...\")\n",
    "        X_test = X_test.dropna()\n",
    "    \n",
    "    # Remove infinite values\n",
    "    X_train = X_train[~np.isinf(X_train.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    X_test = X_test[~np.isinf(X_test.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Total students: {len(df_data)}\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    print(f\"   Training set NaN check: {X_train.isnull().sum().sum()} (should be 0)\")\n",
    "    print(f\"   Test set NaN check: {X_test.isnull().sum().sum()} (should be 0)\")\n",
    "    \n",
    "    # Train Isolation Forest with optimized hyperparameters\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.10,  # Assume 10% anomalies\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_samples='auto',\n",
    "        max_features=1.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"   Contamination rate: 10% (expected anomaly percentage)\")\n",
    "    print(f\"   Number of trees: 100\")\n",
    "    print(f\"   Max features: All\")\n",
    "    \n",
    "    # Train on normalized data\n",
    "    iso_forest.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    train_predictions = iso_forest.predict(X_train)\n",
    "    test_predictions = iso_forest.predict(X_test)\n",
    "    \n",
    "    # Get anomaly scores (lower scores = more anomalous)\n",
    "    train_scores = iso_forest.score_samples(X_train)\n",
    "    test_scores = iso_forest.score_samples(X_test)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà Anomaly Scores (Lower = More Anomalous):\")\n",
    "    print(f\"   Training - Min: {train_scores.min():.4f}, Max: {train_scores.max():.4f}, Mean: {train_scores.mean():.4f}\")\n",
    "    print(f\"   Test - Min: {test_scores.min():.4f}, Max: {test_scores.max():.4f}, Mean: {test_scores.mean():.4f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    iso_forest_results[split_name] = {\n",
    "        'model': iso_forest,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies,\n",
    "        'decision_threshold': iso_forest.offset_\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Isolation Forest training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 CONTINUED: Isolation Forest - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ISOLATION FOREST - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Isolation Forest - Anomaly Scores Distribution (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = iso_forest_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='blue')\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    axes[idx].axvline(iso_forest_results[split_name]['model'].offset_, \n",
    "                      color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    axes[idx].set_xlabel('Anomaly Score')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e9168",
   "metadata": {},
   "source": [
    "## CELL 9: LOCAL OUTLIER FACTOR (LOF) MODEL\n",
    "\n",
    "LOF detects local density-based anomalies by comparing the density of each point with its neighbors. It's effective when anomalies have varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: LOCAL OUTLIER FACTOR (LOF) MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOCAL OUTLIER FACTOR (LOF) - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Density-based anomaly detection\")\n",
    "print(\"Principle: Compares local density with neighbours\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "lof_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training LOF with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Use same splits as Isolation Forest for consistency\n",
    "    X_train = iso_forest_results[split_name]['X_train']\n",
    "    X_test = iso_forest_results[split_name]['X_test']\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    \n",
    "    # Train LOF with optimized hyperparameters\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=20,\n",
    "        contamination=0.10,\n",
    "        novelty=True  # Allow prediction on new data\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"   Number of neighbours: 20\")\n",
    "    print(f\"   Contamination rate: 10%\")\n",
    "    print(f\"   Novelty mode: Enabled\")\n",
    "    \n",
    "    # Train on normalized data\n",
    "    lof.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    train_predictions = lof.predict(X_train)\n",
    "    test_predictions = lof.predict(X_test)\n",
    "    \n",
    "    # Get anomaly scores (lower values = more anomalous)\n",
    "    train_scores = lof.negative_outlier_factor_\n",
    "    test_scores = lof.score_samples(X_test)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà LOF Scores (Lower = More Anomalous):\")\n",
    "    print(f\"   Training - Min: {train_scores.min():.4f}, Max: {train_scores.max():.4f}, Mean: {train_scores.mean():.4f}\")\n",
    "    print(f\"   Test - Min: {test_scores.min():.4f}, Max: {test_scores.max():.4f}, Mean: {test_scores.mean():.4f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    lof_results[split_name] = {\n",
    "        'model': lof,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Local Outlier Factor training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 CONTINUED: LOF - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOF - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Local Outlier Factor (LOF) - Anomaly Scores Distribution (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = lof_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='green')\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    \n",
    "    axes[idx].set_xlabel('LOF Score')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0ceb2",
   "metadata": {},
   "source": [
    "## CELL 10: ONE-CLASS SVM MODEL\n",
    "\n",
    "One-Class SVM learns the boundary of the normal behavior and identifies points outside. It's powerful with high-dimensional data but requires careful hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0df149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: ONE-CLASS SVM MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ONE-CLASS SVM - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Support Vector Machine for single-class classification\")\n",
    "print(\"Principle: Learns boundary of normal behaviour\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "ocsvm_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training One-Class SVM with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Use same splits as Isolation Forest for consistency\n",
    "    X_train = iso_forest_results[split_name]['X_train']\n",
    "    X_test = iso_forest_results[split_name]['X_test']\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    \n",
    "    # Train One-Class SVM with optimized hyperparameters\n",
    "    ocsvm = OneClassSVM(\n",
    "        kernel='rbf',\n",
    "        gamma='auto',\n",
    "        nu=0.10  # Expected fraction of anomalies (10%)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"   Kernel: RBF (Radial Basis Function)\")\n",
    "    print(f\"   Gamma: Auto (1/n_features)\")\n",
    "    print(f\"   Nu (anomaly fraction): 0.10 (10%)\")\n",
    "    \n",
    "    # Train on normalized data\n",
    "    ocsvm.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    train_predictions = ocsvm.predict(X_train)\n",
    "    test_predictions = ocsvm.predict(X_test)\n",
    "    \n",
    "    # Get anomaly scores (distance to hyperplane, negative = anomaly)\n",
    "    train_scores = ocsvm.decision_function(X_train)\n",
    "    test_scores = ocsvm.decision_function(X_test)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà One-Class SVM Decision Function Scores (Negative = Anomaly):\")\n",
    "    print(f\"   Training - Min: {train_scores.min():.4f}, Max: {train_scores.max():.4f}, Mean: {train_scores.mean():.4f}\")\n",
    "    print(f\"   Test - Min: {test_scores.min():.4f}, Max: {test_scores.max():.4f}, Mean: {test_scores.mean():.4f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    ocsvm_results[split_name] = {\n",
    "        'model': ocsvm,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ One-Class SVM training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60796ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 CONTINUED: One-Class SVM - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ONE-CLASS SVM - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('One-Class SVM - Decision Function Scores (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = ocsvm_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='purple')\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    axes[idx].axvline(0, color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    axes[idx].set_xlabel('Decision Function Score')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b78be",
   "metadata": {},
   "source": [
    "## CELL 11: AUTOENCODER MODEL (Deep Learning)\n",
    "\n",
    "Autoencoder detects anomalies based on reconstruction error. Points that are reconstructed poorly are considered anomalies. It captures complex non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0154264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: AUTOENCODER MODEL (DEEP LEARNING)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOENCODER (DEEP LEARNING) - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Neural network (Encoder-Decoder architecture)\")\n",
    "print(\"Principle: Detects anomalies by reconstruction error\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "autoencoder_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training Autoencoder with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Use same splits as Isolation Forest for consistency\n",
    "    X_train_df = iso_forest_results[split_name]['X_train']\n",
    "    X_test_df = iso_forest_results[split_name]['X_test']\n",
    "    \n",
    "    # Convert to numpy arrays for neural network\n",
    "    X_train = X_train_df.values\n",
    "    X_test = X_test_df.values\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    \n",
    "    # Build Autoencoder model\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = int(input_dim / 2)\n",
    "    \n",
    "    # Encoder: Compress input to bottleneck\n",
    "    encoder = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(encoding_dim, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Decoder: Reconstruct from bottleneck\n",
    "    decoder = keras.Sequential([\n",
    "        layers.Dense(32, activation='relu', input_shape=(encoding_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Full Autoencoder\n",
    "    autoencoder = keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  Autoencoder Architecture:\")\n",
    "    print(f\"   Input dimension: {input_dim} variables\")\n",
    "    print(f\"   Layer 1: 64 neurons (ReLU)\")\n",
    "    print(f\"   Layer 2: 32 neurons (ReLU)\")\n",
    "    print(f\"   Bottleneck (Encoder output): {encoding_dim} neurons (ReLU)\")\n",
    "    print(f\"   Layer 4: 32 neurons (ReLU)\")\n",
    "    print(f\"   Layer 5: 64 neurons (ReLU)\")\n",
    "    print(f\"   Output: {input_dim} neurons (Sigmoid)\")\n",
    "    print(f\"   Total parameters: {autoencoder.count_params():,}\")\n",
    "    \n",
    "    # Train Autoencoder\n",
    "    print(f\"\\nüìö Training Autoencoder...\")\n",
    "    print(f\"   Epochs: 50 | Batch size: 32 | Validation split: 10%\")\n",
    "    \n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"   Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"   Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    train_reconstructions = autoencoder.predict(X_train, verbose=0)\n",
    "    test_reconstructions = autoencoder.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Mean Squared Error for each sample\n",
    "    train_mse = np.mean(np.square(X_train - train_reconstructions), axis=1)\n",
    "    test_mse = np.mean(np.square(X_test - test_reconstructions), axis=1)\n",
    "    \n",
    "    # Calculate threshold (95th percentile of training MSE)\n",
    "    threshold = np.percentile(train_mse, 95)\n",
    "    \n",
    "    # Classify anomalies based on reconstruction error\n",
    "    train_predictions = np.where(train_mse > threshold, -1, 1)\n",
    "    test_predictions = np.where(test_mse > threshold, -1, 1)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà Reconstruction Error (Mean Squared Error):\")\n",
    "    print(f\"   Training - Min: {train_mse.min():.6f}, Max: {train_mse.max():.6f}, Mean: {train_mse.mean():.6f}\")\n",
    "    print(f\"   Test - Min: {test_mse.min():.6f}, Max: {test_mse.max():.6f}, Mean: {test_mse.mean():.6f}\")\n",
    "    print(f\"   Anomaly threshold (95th percentile): {threshold:.6f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    autoencoder_results[split_name] = {\n",
    "        'model': autoencoder,\n",
    "        'encoder': encoder,\n",
    "        'decoder': decoder,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies,\n",
    "        'threshold': threshold,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Autoencoder training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 CONTINUED: Autoencoder - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOENCODER - RECONSTRUCTION ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Autoencoder - Reconstruction Error Distribution (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = autoencoder_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_mse'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='orange')\n",
    "    axes[idx].hist(results['train_mse'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    axes[idx].axvline(results['threshold'], color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "    \n",
    "    axes[idx].set_xlabel('Reconstruction Error (MSE)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd128d",
   "metadata": {},
   "source": [
    "## COMPARISON: All Models Performance Across Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359029da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS COMPARISON\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison summary\n",
    "comparison_data = []\n",
    "\n",
    "for split_name in ['70/30', '60/40', '80/20']:\n",
    "    # Isolation Forest\n",
    "    if_results = iso_forest_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'Isolation Forest',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': if_results['train_anomalies'],\n",
    "        'Test_Anomalies': if_results['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * if_results['train_anomalies'] / len(if_results['X_train']),\n",
    "        'Test_Rate_%': 100 * if_results['test_anomalies'] / len(if_results['X_test'])\n",
    "    })\n",
    "    \n",
    "    # LOF\n",
    "    lof = lof_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'LOF',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': lof['train_anomalies'],\n",
    "        'Test_Anomalies': lof['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * lof['train_anomalies'] / len(lof['X_train']),\n",
    "        'Test_Rate_%': 100 * lof['test_anomalies'] / len(lof['X_test'])\n",
    "    })\n",
    "    \n",
    "    # One-Class SVM\n",
    "    ocsvm = ocsvm_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'One-Class SVM',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': ocsvm['train_anomalies'],\n",
    "        'Test_Anomalies': ocsvm['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * ocsvm['train_anomalies'] / len(ocsvm['X_train']),\n",
    "        'Test_Rate_%': 100 * ocsvm['test_anomalies'] / len(ocsvm['X_test'])\n",
    "    })\n",
    "    \n",
    "    # Autoencoder\n",
    "    ae = autoencoder_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'Autoencoder',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': ae['train_anomalies'],\n",
    "        'Test_Anomalies': ae['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * ae['train_anomalies'] / len(ae['X_train']),\n",
    "        'Test_Rate_%': 100 * ae['test_anomalies'] / len(ae['X_test'])\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìä ANOMALY DETECTION RESULTS ACROSS ALL MODEL-SPLIT COMBINATIONS:\")\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n‚ú® KEY INSIGHTS:\")\n",
    "print(\"\\n1Ô∏è‚É£  ISOLATION FOREST:\")\n",
    "if_avg = iso_forest_results['70/30']['test_anomalies'] + iso_forest_results['60/40']['test_anomalies'] + iso_forest_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {if_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='Isolation Forest'].loc[comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].idxmax(), 'Split']}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  LOCAL OUTLIER FACTOR (LOF):\")\n",
    "lof_avg = lof_results['70/30']['test_anomalies'] + lof_results['60/40']['test_anomalies'] + lof_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {lof_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='LOF'].loc[comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].idxmax(), 'Split']}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  ONE-CLASS SVM:\")\n",
    "ocsvm_avg = ocsvm_results['70/30']['test_anomalies'] + ocsvm_results['60/40']['test_anomalies'] + ocsvm_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {ocsvm_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='One-Class SVM'].loc[comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].idxmax(), 'Split']}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  AUTOENCODER:\")\n",
    "ae_avg = autoencoder_results['70/30']['test_anomalies'] + autoencoder_results['60/40']['test_anomalies'] + autoencoder_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {ae_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='Autoencoder'].loc[comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].idxmax(), 'Split']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle('Model Performance Comparison Across Train/Test Splits', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Test anomalies count\n",
    "split_order = ['70/30', '60/40', '80/20']\n",
    "models = ['Isolation Forest', 'LOF', 'One-Class SVM', 'Autoencoder']\n",
    "for model in models:\n",
    "    model_data = comparison_df[comparison_df['Model'] == model].sort_values('Split', key=lambda x: x.map({s: i for i, s in enumerate(split_order)}))\n",
    "    axes[0].plot(model_data['Split'], model_data['Test_Anomalies'], marker='o', label=model, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('Train/Test Split', fontweight='bold')\n",
    "axes[0].set_ylabel('Anomalies Detected (Test Set)', fontweight='bold')\n",
    "axes[0].set_title('Absolute Anomaly Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Test anomaly rate\n",
    "for model in models:\n",
    "    model_data = comparison_df[comparison_df['Model'] == model].sort_values('Split', key=lambda x: x.map({s: i for i, s in enumerate(split_order)}))\n",
    "    axes[1].plot(model_data['Split'], model_data['Test_Rate_%'], marker='s', label=model, linewidth=2, markersize=8)\n",
    "\n",
    "axes[1].set_xlabel('Train/Test Split', fontweight='bold')\n",
    "axes[1].set_ylabel('Detection Rate (%)', fontweight='bold')\n",
    "axes[1].set_title('Anomaly Detection Rate')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Comparison visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c500c",
   "metadata": {},
   "source": [
    "## FINAL RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 3 FINAL SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3 SUCCESSFULLY COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéâ MODELS TRAINED AND EVALUATED:\n",
    "\n",
    "‚úÖ Isolation Forest - 3 different train/test splits\n",
    "‚úÖ Local Outlier Factor (LOF) - 3 different train/test splits\n",
    "‚úÖ One-Class SVM - 3 different train/test splits\n",
    "‚úÖ Autoencoder (Deep Learning) - 3 different train/test splits\n",
    "\n",
    "üìä TRAIN/TEST SPLITS EVALUATED:\n",
    "   ‚Ä¢ 70% Training / 30% Testing\n",
    "   ‚Ä¢ 60% Training / 40% Testing\n",
    "   ‚Ä¢ 80% Training / 20% Testing\n",
    "\n",
    "üîç TOTAL STUDENTS ANALYZED:\n",
    "   ‚Ä¢ Isolation Forest anomalies: {comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Anomalies'].sum()}\n",
    "   ‚Ä¢ LOF anomalies: {comparison_df[comparison_df['Model']=='LOF']['Test_Anomalies'].sum()}\n",
    "   ‚Ä¢ One-Class SVM anomalies: {comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Anomalies'].sum()}\n",
    "   ‚Ä¢ Autoencoder anomalies: {comparison_df[comparison_df['Model']=='Autoencoder']['Test_Anomalies'].sum()}\n",
    "\n",
    "üìà DETECTION RATES (Average across all splits):\n",
    "   ‚Ä¢ Isolation Forest: {comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].mean():.2f}%\n",
    "   ‚Ä¢ LOF: {comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].mean():.2f}%\n",
    "   ‚Ä¢ One-Class SVM: {comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].mean():.2f}%\n",
    "   ‚Ä¢ Autoencoder: {comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].mean():.2f}%\n",
    "\n",
    "üèÜ BEST PERFORMING CONFIGURATIONS:\n",
    "   ‚Ä¢ Isolation Forest: {comparison_df[comparison_df['Model']=='Isolation Forest'].loc[comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "   ‚Ä¢ LOF: {comparison_df[comparison_df['Model']=='LOF'].loc[comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "   ‚Ä¢ One-Class SVM: {comparison_df[comparison_df['Model']=='One-Class SVM'].loc[comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "   ‚Ä¢ Autoencoder: {comparison_df[comparison_df['Model']=='Autoencoder'].loc[comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "\n",
    "‚ú® Next Phase: Phase 4 - VALIDATION (Consensus and Analysis)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Ready for Phase 4: VALIDATION AND CONSENSUS ANALYSIS\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
