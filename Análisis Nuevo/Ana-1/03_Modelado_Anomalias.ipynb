{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3adc24aa",
   "metadata": {},
   "source": [
    "# üîç ANOMALY DETECTION IN MAMBA SEEDLING STUDENTS\n",
    "## Phase 3: MODELING (Implement Anomaly Detection Algorithms)\n",
    "\n",
    "---\n",
    "\n",
    "### OBJECTIVES OF THIS PHASE:\n",
    "1. Load prepared and normalized data\n",
    "2. Implement 4 anomaly detection algorithms\n",
    "3. Test with 3 different train/test splits (70/30, 60/40, 80/20)\n",
    "4. Compare performance across different splits\n",
    "5. Identify best configuration for each model\n",
    "\n",
    "### EXPECTED OUTPUT:\n",
    "- 4 trained anomaly detection models\n",
    "- Performance metrics for each train/test split\n",
    "- Detected anomalies flagged and scored\n",
    "- Comparison analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea83b6",
   "metadata": {},
   "source": [
    "## INITIAL SETUP: Load Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf58d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "\n",
      "üìä PREPARED DATA LOADED FROM PHASE 2\n",
      "   File: dataset_prepared_minmax.csv\n",
      "   Samples (students): 81\n",
      "   Variables (features): 45\n",
      "   Data normalization: Min-Max [0, 1]\n",
      "\n",
      "üîç DATA VALIDATION:\n",
      "   Missing values (NaN): 243\n",
      "   ‚ö†Ô∏è  Found NaN values! Removing rows with missing data...\n",
      "   Removed 81 rows with NaN\n",
      "   Remaining samples: 0\n",
      "   ‚úÖ Data is CLEAN - No NaN or infinite values\n",
      "   Data type validation:\n",
      "   - Numeric columns: 45\n",
      "   - All values in [0, 1]? True\n",
      "\n",
      "‚úÖ Min-Max Scaler loaded (feature_range=(0, 1))\n",
      "\n",
      "üìä Dataset Preview (First 5 rows):\n",
      "Empty DataFrame\n",
      "Columns: [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11, Q12, Q13, Q14, Q15, Q16, Q17, Q18, Q19, Q20, Q21, Q22, Q23, Q24, Q25, Q26, Q27, Q28, Q29, Q30, Q31, Q32, Q33, Q34, Q35, F_Average_Performance, F_Academic_Load, F_Life_Balance, F_Psychological_Stress, F_Family_Support, F_Grade_Consistency, F_Responsibility_Result_Index, F_Parental_Education, F_Socioeconomic_Risk, F_Interest_Performance_Gap]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 45 columns]\n",
      "\n",
      "üìà Data Statistics:\n",
      "   Min value (all variables): nan\n",
      "   Max value (all variables): nan\n",
      "   Mean value (all variables): nan\n",
      "   Std value (all variables): nan\n",
      "\n",
      "‚ú® Data ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# SETUP: Import libraries and load prepared data from Phase 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import anomaly detection models\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, silhouette_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD PREPARED DATA FROM PHASE 2\n",
    "# ============================================================================\n",
    "data_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\An√°lisis Nuevo\\data\\dataset_prepared_minmax.csv'\n",
    "df_data = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\nüìä PREPARED DATA LOADED FROM PHASE 2\")\n",
    "print(f\"   File: dataset_prepared_minmax.csv\")\n",
    "print(f\"   Samples (students): {df_data.shape[0]}\")\n",
    "print(f\"   Variables (features): {df_data.shape[1]}\")\n",
    "print(f\"   Data normalization: Min-Max [0, 1]\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA VALIDATION AND INTELLIGENT CLEANING\n",
    "# ============================================================================\n",
    "print(f\"\\nüîç DATA VALIDATION AND CLEANING:\")\n",
    "\n",
    "initial_rows = len(df_data)\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df_data.isnull().sum().sum()\n",
    "print(f\"   1. Missing values (NaN): {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"      ‚ö†Ô∏è  Found {missing_count} NaN values in data\")\n",
    "    print(f\"      Strategy: Using SimpleImputer with median strategy\")\n",
    "    \n",
    "    # Use median imputation instead of dropping rows\n",
    "    numeric_cols = df_data.select_dtypes(include=[np.number]).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_data[numeric_cols] = imputer.fit_transform(df_data[numeric_cols])\n",
    "    \n",
    "    print(f\"      ‚úÖ NaN values imputed with median\")\n",
    "\n",
    "# Check for infinite values\n",
    "has_inf = np.isinf(df_data.select_dtypes(include=[np.number]).values).any()\n",
    "if has_inf:\n",
    "    print(f\"   2. Infinite values detected\")\n",
    "    print(f\"      ‚ö†Ô∏è  Replacing infinite values with max/min bounds...\")\n",
    "    \n",
    "    numeric_cols = df_data.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df_data[col] = df_data[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Impute the remaining NaN from infinite replacement\n",
    "    df_data[numeric_cols] = imputer.fit_transform(df_data[numeric_cols])\n",
    "    print(f\"      ‚úÖ Infinite values fixed\")\n",
    "else:\n",
    "    print(f\"   2. Infinite values: None detected\")\n",
    "\n",
    "# Check data types are numeric\n",
    "print(f\"   3. Data type validation:\")\n",
    "print(f\"      - Numeric columns: {len(df_data.select_dtypes(include=[np.number]).columns)}\")\n",
    "\n",
    "# Verify data is in expected range\n",
    "numeric_df = df_data.select_dtypes(include=[np.number])\n",
    "min_val = numeric_df.min().min()\n",
    "max_val = numeric_df.max().max()\n",
    "print(f\"      - Value range: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "\n",
    "# Final validation\n",
    "final_missing = df_data.isnull().sum().sum()\n",
    "final_inf = np.isinf(df_data.select_dtypes(include=[np.number]).values).any()\n",
    "\n",
    "print(f\"\\n   ‚úÖ DATA VALIDATION COMPLETE:\")\n",
    "print(f\"      Initial rows: {initial_rows}\")\n",
    "print(f\"      Final rows: {len(df_data)} (0 dropped)\")\n",
    "print(f\"      Missing values after cleaning: {final_missing}\")\n",
    "print(f\"      Infinite values after cleaning: {final_inf}\")\n",
    "\n",
    "if len(df_data) == 0:\n",
    "    print(f\"\\n   ‚ùå ERROR: Dataset is empty! Check source data.\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Dataset is ready for modeling\")\n",
    "\n",
    "# Load Min-Max Scaler for reference\n",
    "scaler_path = r'c:\\Users\\DELL\\Documents\\GitHub\\material-DT-1\\An√°lisis Nuevo\\data\\scaler_minmax.pkl'\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler_minmax = pickle.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ Min-Max Scaler loaded (feature_range={scaler_minmax.feature_range})\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Preview (First 5 rows):\")\n",
    "print(df_data.head())\n",
    "\n",
    "print(f\"\\nüìà Data Statistics:\")\n",
    "numeric_cols = df_data.select_dtypes(include=[np.number])\n",
    "print(f\"   Min value (all variables): {numeric_cols.min().min():.6f}\")\n",
    "print(f\"   Max value (all variables): {numeric_cols.max().max():.6f}\")\n",
    "print(f\"   Mean value (all variables): {numeric_cols.mean().mean():.6f}\")\n",
    "print(f\"   Std value (all variables): {numeric_cols.std().mean():.6f}\")\n",
    "\n",
    "print(f\"\\n‚ú® Data ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57356c",
   "metadata": {},
   "source": [
    "## CELL 8: ISOLATION FOREST MODEL\n",
    "\n",
    "Isolation Forest is an ensemble method that isolates anomalies by randomly selecting features and split values. It's efficient and doesn't require distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bf454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ISOLATION FOREST - ANOMALY DETECTION\n",
      "================================================================================\n",
      "\n",
      "Algorithm: Ensemble method for anomaly detection\n",
      "Principle: Isolates anomalies by randomly selecting features\n",
      "Features used: All prepared and normalized variables from Phase 2\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Training Isolation Forest with 70/30 split:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=None and train_size=0.7, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m‚îÄ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Split data using the prepared normalized dataset\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# ADDITIONAL DATA VALIDATION AFTER SPLIT\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Remove any rows with NaN that might have slipped through\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2419\u001b[0m     )\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=None and train_size=0.7, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# CELL 8: ISOLATION FOREST MODEL\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ISOLATION FOREST - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Ensemble method for anomaly detection\")\n",
    "print(\"Principle: Isolates anomalies by randomly selecting features\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "# Define train/test split configurations\n",
    "splits_config = [\n",
    "    {'train_size': 0.70, 'test_size': 0.30, 'name': '70/30'},\n",
    "    {'train_size': 0.60, 'test_size': 0.40, 'name': '60/40'},\n",
    "    {'train_size': 0.80, 'test_size': 0.20, 'name': '80/20'}\n",
    "]\n",
    "\n",
    "iso_forest_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training Isolation Forest with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Split data using the prepared normalized dataset\n",
    "    X_train, X_test = train_test_split(\n",
    "        df_data, \n",
    "        train_size=train_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # LIGHT DATA VALIDATION AFTER SPLIT\n",
    "    # Only handle critical issues, don't drop rows\n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Imputing NaN in training set with median...\")\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
    "    \n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Imputing NaN in test set with median...\")\n",
    "        numeric_cols = X_test.select_dtypes(include=[np.number]).columns\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_test[numeric_cols] = imputer.fit_transform(X_test[numeric_cols])\n",
    "    \n",
    "    # Handle infinite values by bounding them\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
    "    \n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        numeric_cols = X_test.select_dtypes(include=[np.number]).columns\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_test[numeric_cols] = imputer.fit_transform(X_test[numeric_cols])\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Total students: {len(df_data)}\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    print(f\"   Training set - Missing values: {X_train.isnull().sum().sum()} (after handling)\")\n",
    "    print(f\"   Test set - Missing values: {X_test.isnull().sum().sum()} (after handling)\")\n",
    "    \n",
    "    # Train Isolation Forest with optimized hyperparameters\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.10,  # Assume 10% anomalies\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_samples='auto',\n",
    "        max_features=1.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"   Contamination rate: 10% (expected anomaly percentage)\")\n",
    "    print(f\"   Number of trees: 100\")\n",
    "    print(f\"   Max features: All\")\n",
    "    \n",
    "    # Train on normalized data\n",
    "    iso_forest.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    train_predictions = iso_forest.predict(X_train)\n",
    "    test_predictions = iso_forest.predict(X_test)\n",
    "    \n",
    "    # Get anomaly scores (lower scores = more anomalous)\n",
    "    train_scores = iso_forest.score_samples(X_train)\n",
    "    test_scores = iso_forest.score_samples(X_test)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà Anomaly Scores (Lower = More Anomalous):\")\n",
    "    print(f\"   Training - Min: {train_scores.min():.4f}, Max: {train_scores.max():.4f}, Mean: {train_scores.mean():.4f}\")\n",
    "    print(f\"   Test - Min: {test_scores.min():.4f}, Max: {test_scores.max():.4f}, Mean: {test_scores.mean():.4f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    iso_forest_results[split_name] = {\n",
    "        'model': iso_forest,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies,\n",
    "        'decision_threshold': iso_forest.offset_\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Isolation Forest training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 CONTINUED: Isolation Forest - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ISOLATION FOREST - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Isolation Forest - Anomaly Scores Distribution (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = iso_forest_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='blue')\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    axes[idx].axvline(iso_forest_results[split_name]['model'].offset_, \n",
    "                      color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    axes[idx].set_xlabel('Anomaly Score')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e9168",
   "metadata": {},
   "source": [
    "## CELL 9: LOCAL OUTLIER FACTOR (LOF) MODEL\n",
    "\n",
    "LOF detects local density-based anomalies by comparing the density of each point with its neighbors. It's effective when anomalies have varying densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: LOCAL OUTLIER FACTOR (LOF) MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOCAL OUTLIER FACTOR (LOF) - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Density-based anomaly detection\")\n",
    "print(\"Principle: Compares local density with neighbours\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "lof_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training LOF with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Use same splits as Isolation Forest for consistency\n",
    "    X_train = iso_forest_results[split_name]['X_train'].copy()\n",
    "    X_test = iso_forest_results[split_name]['X_test'].copy()\n",
    "    \n",
    "    # ADDITIONAL DATA VALIDATION\n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from training set...\")\n",
    "        X_train = X_train.dropna()\n",
    "    \n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from test set...\")\n",
    "        X_test = X_test.dropna()\n",
    "    \n",
    "    # Remove infinite values\n",
    "    X_train = X_train[~np.isinf(X_train.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    X_test = X_test[~np.isinf(X_test.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    print(f\"   Data quality: {X_train.isnull().sum().sum()} NaN values\")\n",
    "    \n",
    "    # Train LOF with optimized hyperparameters\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=20,\n",
    "        contamination=0.10,\n",
    "        novelty=True  # Allow prediction on new data\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"   Number of neighbours: 20\")\n",
    "    print(f\"   Contamination rate: 10%\")\n",
    "    print(f\"   Novelty mode: Enabled\")\n",
    "    \n",
    "    # Train on normalized data\n",
    "    lof.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    train_predictions = lof.predict(X_train)\n",
    "    test_predictions = lof.predict(X_test)\n",
    "    \n",
    "    # Get anomaly scores (lower values = more anomalous)\n",
    "    train_scores = lof.negative_outlier_factor_\n",
    "    test_scores = lof.score_samples(X_test)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà LOF Scores (Lower = More Anomalous):\")\n",
    "    print(f\"   Training - Min: {train_scores.min():.4f}, Max: {train_scores.max():.4f}, Mean: {train_scores.mean():.4f}\")\n",
    "    print(f\"   Test - Min: {test_scores.min():.4f}, Max: {test_scores.max():.4f}, Mean: {test_scores.mean():.4f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    lof_results[split_name] = {\n",
    "        'model': lof,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Local Outlier Factor training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 CONTINUED: LOF - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOF - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Local Outlier Factor (LOF) - Anomaly Scores Distribution (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = lof_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='green')\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    \n",
    "    axes[idx].set_xlabel('LOF Score')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0ceb2",
   "metadata": {},
   "source": [
    "## CELL 10: ONE-CLASS SVM MODEL\n",
    "\n",
    "One-Class SVM learns the boundary of the normal behavior and identifies points outside. It's powerful with high-dimensional data but requires careful hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0df149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: ONE-CLASS SVM MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ONE-CLASS SVM - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Support Vector Machine for single-class classification\")\n",
    "print(\"Principle: Learns boundary of normal behaviour\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "ocsvm_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training One-Class SVM with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Use same splits as Isolation Forest for consistency\n",
    "    X_train = iso_forest_results[split_name]['X_train'].copy()\n",
    "    X_test = iso_forest_results[split_name]['X_test'].copy()\n",
    "    \n",
    "    # ADDITIONAL DATA VALIDATION\n",
    "    if X_train.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from training set...\")\n",
    "        X_train = X_train.dropna()\n",
    "    \n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from test set...\")\n",
    "        X_test = X_test.dropna()\n",
    "    \n",
    "    # Remove infinite values\n",
    "    X_train = X_train[~np.isinf(X_train.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    X_test = X_test[~np.isinf(X_test.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    print(f\"   Data quality: {X_train.isnull().sum().sum()} NaN values\")\n",
    "    \n",
    "    # Train One-Class SVM with optimized hyperparameters\n",
    "    ocsvm = OneClassSVM(\n",
    "        kernel='rbf',\n",
    "        gamma='auto',\n",
    "        nu=0.10  # Expected fraction of anomalies (10%)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"   Kernel: RBF (Radial Basis Function)\")\n",
    "    print(f\"   Gamma: Auto (1/n_features)\")\n",
    "    print(f\"   Nu (anomaly fraction): 0.10 (10%)\")\n",
    "    \n",
    "    # Train on normalized data\n",
    "    ocsvm.fit(X_train)\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    train_predictions = ocsvm.predict(X_train)\n",
    "    test_predictions = ocsvm.predict(X_test)\n",
    "    \n",
    "    # Get anomaly scores (distance to hyperplane, negative = anomaly)\n",
    "    train_scores = ocsvm.decision_function(X_train)\n",
    "    test_scores = ocsvm.decision_function(X_test)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà One-Class SVM Decision Function Scores (Negative = Anomaly):\")\n",
    "    print(f\"   Training - Min: {train_scores.min():.4f}, Max: {train_scores.max():.4f}, Mean: {train_scores.mean():.4f}\")\n",
    "    print(f\"   Test - Min: {test_scores.min():.4f}, Max: {test_scores.max():.4f}, Mean: {test_scores.mean():.4f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    ocsvm_results[split_name] = {\n",
    "        'model': ocsvm,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ One-Class SVM training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60796ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 CONTINUED: One-Class SVM - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ONE-CLASS SVM - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('One-Class SVM - Decision Function Scores (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = ocsvm_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='purple')\n",
    "    axes[idx].hist(results['train_scores'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    axes[idx].axvline(0, color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    axes[idx].set_xlabel('Decision Function Score')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b78be",
   "metadata": {},
   "source": [
    "## CELL 11: AUTOENCODER MODEL (Deep Learning)\n",
    "\n",
    "Autoencoder detects anomalies based on reconstruction error. Points that are reconstructed poorly are considered anomalies. It captures complex non-linear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0154264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: AUTOENCODER MODEL (DEEP LEARNING)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOENCODER (DEEP LEARNING) - ANOMALY DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAlgorithm: Neural network (Encoder-Decoder architecture)\")\n",
    "print(\"Principle: Detects anomalies by reconstruction error\")\n",
    "print(\"Features used: All prepared and normalized variables from Phase 2\")\n",
    "\n",
    "autoencoder_results = {}\n",
    "\n",
    "for config in splits_config:\n",
    "    train_size = config['train_size']\n",
    "    split_name = config['name']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Training Autoencoder with {split_name} split:\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Use same splits as Isolation Forest for consistency\n",
    "    X_train_df = iso_forest_results[split_name]['X_train'].copy()\n",
    "    X_test_df = iso_forest_results[split_name]['X_test'].copy()\n",
    "    \n",
    "    # ADDITIONAL DATA VALIDATION\n",
    "    if X_train_df.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from training set...\")\n",
    "        X_train_df = X_train_df.dropna()\n",
    "    \n",
    "    if X_test_df.isnull().sum().sum() > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Cleaning NaN from test set...\")\n",
    "        X_test_df = X_test_df.dropna()\n",
    "    \n",
    "    # Remove infinite values\n",
    "    X_train_df = X_train_df[~np.isinf(X_train_df.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    X_test_df = X_test_df[~np.isinf(X_test_df.select_dtypes(include=[np.number])).any(axis=1)]\n",
    "    \n",
    "    # Convert to numpy arrays for neural network\n",
    "    X_train = X_train_df.values\n",
    "    X_test = X_test_df.values\n",
    "    \n",
    "    print(f\"\\nüìä Data Split (from prepared dataset):\")\n",
    "    print(f\"   Training samples: {len(X_train)} ({100*len(X_train)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Test samples: {len(X_test)} ({100*len(X_test)/len(df_data):.1f}%)\")\n",
    "    print(f\"   Features used: {X_train.shape[1]} variables\")\n",
    "    print(f\"   Data quality: NaN={np.isnan(X_train).sum()}, Inf={np.isinf(X_train).sum()}\")\n",
    "    \n",
    "    # Build Autoencoder model\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = int(input_dim / 2)\n",
    "    \n",
    "    # Encoder: Compress input to bottleneck\n",
    "    encoder = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(encoding_dim, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Decoder: Reconstruct from bottleneck\n",
    "    decoder = keras.Sequential([\n",
    "        layers.Dense(32, activation='relu', input_shape=(encoding_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Full Autoencoder\n",
    "    autoencoder = keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  Autoencoder Architecture:\")\n",
    "    print(f\"   Input dimension: {input_dim} variables\")\n",
    "    print(f\"   Layer 1: 64 neurons (ReLU)\")\n",
    "    print(f\"   Layer 2: 32 neurons (ReLU)\")\n",
    "    print(f\"   Bottleneck (Encoder output): {encoding_dim} neurons (ReLU)\")\n",
    "    print(f\"   Layer 4: 32 neurons (ReLU)\")\n",
    "    print(f\"   Layer 5: 64 neurons (ReLU)\")\n",
    "    print(f\"   Output: {input_dim} neurons (Sigmoid)\")\n",
    "    print(f\"   Total parameters: {autoencoder.count_params():,}\")\n",
    "    \n",
    "    # Train Autoencoder\n",
    "    print(f\"\\nüìö Training Autoencoder...\")\n",
    "    print(f\"   Epochs: 50 | Batch size: 32 | Validation split: 10%\")\n",
    "    \n",
    "    history = autoencoder.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"   Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"   Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    train_reconstructions = autoencoder.predict(X_train, verbose=0)\n",
    "    test_reconstructions = autoencoder.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Mean Squared Error for each sample\n",
    "    train_mse = np.mean(np.square(X_train - train_reconstructions), axis=1)\n",
    "    test_mse = np.mean(np.square(X_test - test_reconstructions), axis=1)\n",
    "    \n",
    "    # Calculate threshold (95th percentile of training MSE)\n",
    "    threshold = np.percentile(train_mse, 95)\n",
    "    \n",
    "    # Classify anomalies based on reconstruction error\n",
    "    train_predictions = np.where(train_mse > threshold, -1, 1)\n",
    "    test_predictions = np.where(test_mse > threshold, -1, 1)\n",
    "    \n",
    "    # Count anomalies\n",
    "    train_anomalies = (train_predictions == -1).sum()\n",
    "    test_anomalies = (test_predictions == -1).sum()\n",
    "    \n",
    "    print(f\"\\nüîç Anomalies Detected:\")\n",
    "    print(f\"   Training set: {train_anomalies} anomalies ({100*train_anomalies/len(X_train):.2f}%)\")\n",
    "    print(f\"   Test set: {test_anomalies} anomalies ({100*test_anomalies/len(X_test):.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà Reconstruction Error (Mean Squared Error):\")\n",
    "    print(f\"   Training - Min: {train_mse.min():.6f}, Max: {train_mse.max():.6f}, Mean: {train_mse.mean():.6f}\")\n",
    "    print(f\"   Test - Min: {test_mse.min():.6f}, Max: {test_mse.max():.6f}, Mean: {test_mse.mean():.6f}\")\n",
    "    print(f\"   Anomaly threshold (95th percentile): {threshold:.6f}\")\n",
    "    \n",
    "    # Store results with additional metadata\n",
    "    autoencoder_results[split_name] = {\n",
    "        'model': autoencoder,\n",
    "        'encoder': encoder,\n",
    "        'decoder': decoder,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'train_predictions': train_predictions,\n",
    "        'test_predictions': test_predictions,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'test_anomalies': test_anomalies,\n",
    "        'threshold': threshold,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Autoencoder training completed for all splits (70/30, 60/40, 80/20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 CONTINUED: Autoencoder - Visualization\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOENCODER - RECONSTRUCTION ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('Autoencoder - Reconstruction Error Distribution (3 Train/Test Splits)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for idx, split_name in enumerate(['70/30', '60/40', '80/20']):\n",
    "    results = autoencoder_results[split_name]\n",
    "    \n",
    "    # Plot distributions\n",
    "    axes[idx].hist(results['train_mse'][results['train_predictions'] == 1], \n",
    "                   bins=20, alpha=0.6, label='Normal (Train)', color='orange')\n",
    "    axes[idx].hist(results['train_mse'][results['train_predictions'] == -1], \n",
    "                   bins=20, alpha=0.6, label='Anomaly (Train)', color='red')\n",
    "    axes[idx].axvline(results['threshold'], color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "    \n",
    "    axes[idx].set_xlabel('Reconstruction Error (MSE)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Split {split_name}\\n(Train: {len(results[\"X_train\"])}, Test: {len(results[\"X_test\"])})')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd128d",
   "metadata": {},
   "source": [
    "## COMPARISON: All Models Performance Across Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359029da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS COMPARISON\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison summary\n",
    "comparison_data = []\n",
    "\n",
    "for split_name in ['70/30', '60/40', '80/20']:\n",
    "    # Isolation Forest\n",
    "    if_results = iso_forest_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'Isolation Forest',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': if_results['train_anomalies'],\n",
    "        'Test_Anomalies': if_results['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * if_results['train_anomalies'] / len(if_results['X_train']),\n",
    "        'Test_Rate_%': 100 * if_results['test_anomalies'] / len(if_results['X_test'])\n",
    "    })\n",
    "    \n",
    "    # LOF\n",
    "    lof = lof_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'LOF',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': lof['train_anomalies'],\n",
    "        'Test_Anomalies': lof['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * lof['train_anomalies'] / len(lof['X_train']),\n",
    "        'Test_Rate_%': 100 * lof['test_anomalies'] / len(lof['X_test'])\n",
    "    })\n",
    "    \n",
    "    # One-Class SVM\n",
    "    ocsvm = ocsvm_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'One-Class SVM',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': ocsvm['train_anomalies'],\n",
    "        'Test_Anomalies': ocsvm['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * ocsvm['train_anomalies'] / len(ocsvm['X_train']),\n",
    "        'Test_Rate_%': 100 * ocsvm['test_anomalies'] / len(ocsvm['X_test'])\n",
    "    })\n",
    "    \n",
    "    # Autoencoder\n",
    "    ae = autoencoder_results[split_name]\n",
    "    comparison_data.append({\n",
    "        'Model': 'Autoencoder',\n",
    "        'Split': split_name,\n",
    "        'Train_Anomalies': ae['train_anomalies'],\n",
    "        'Test_Anomalies': ae['test_anomalies'],\n",
    "        'Train_Rate_%': 100 * ae['train_anomalies'] / len(ae['X_train']),\n",
    "        'Test_Rate_%': 100 * ae['test_anomalies'] / len(ae['X_test'])\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìä ANOMALY DETECTION RESULTS ACROSS ALL MODEL-SPLIT COMBINATIONS:\")\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n‚ú® KEY INSIGHTS:\")\n",
    "print(\"\\n1Ô∏è‚É£  ISOLATION FOREST:\")\n",
    "if_avg = iso_forest_results['70/30']['test_anomalies'] + iso_forest_results['60/40']['test_anomalies'] + iso_forest_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {if_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='Isolation Forest'].loc[comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].idxmax(), 'Split']}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  LOCAL OUTLIER FACTOR (LOF):\")\n",
    "lof_avg = lof_results['70/30']['test_anomalies'] + lof_results['60/40']['test_anomalies'] + lof_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {lof_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='LOF'].loc[comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].idxmax(), 'Split']}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  ONE-CLASS SVM:\")\n",
    "ocsvm_avg = ocsvm_results['70/30']['test_anomalies'] + ocsvm_results['60/40']['test_anomalies'] + ocsvm_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {ocsvm_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='One-Class SVM'].loc[comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].idxmax(), 'Split']}\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  AUTOENCODER:\")\n",
    "ae_avg = autoencoder_results['70/30']['test_anomalies'] + autoencoder_results['60/40']['test_anomalies'] + autoencoder_results['80/20']['test_anomalies']\n",
    "print(f\"    Total anomalies detected (test sets): {ae_avg}\")\n",
    "print(f\"    Average detection rate: {comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].mean():.2f}%\")\n",
    "print(f\"    Best split: {comparison_df[comparison_df['Model']=='Autoencoder'].loc[comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].idxmax(), 'Split']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle('Model Performance Comparison Across Train/Test Splits', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Test anomalies count\n",
    "split_order = ['70/30', '60/40', '80/20']\n",
    "models = ['Isolation Forest', 'LOF', 'One-Class SVM', 'Autoencoder']\n",
    "for model in models:\n",
    "    model_data = comparison_df[comparison_df['Model'] == model].sort_values('Split', key=lambda x: x.map({s: i for i, s in enumerate(split_order)}))\n",
    "    axes[0].plot(model_data['Split'], model_data['Test_Anomalies'], marker='o', label=model, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('Train/Test Split', fontweight='bold')\n",
    "axes[0].set_ylabel('Anomalies Detected (Test Set)', fontweight='bold')\n",
    "axes[0].set_title('Absolute Anomaly Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Test anomaly rate\n",
    "for model in models:\n",
    "    model_data = comparison_df[comparison_df['Model'] == model].sort_values('Split', key=lambda x: x.map({s: i for i, s in enumerate(split_order)}))\n",
    "    axes[1].plot(model_data['Split'], model_data['Test_Rate_%'], marker='s', label=model, linewidth=2, markersize=8)\n",
    "\n",
    "axes[1].set_xlabel('Train/Test Split', fontweight='bold')\n",
    "axes[1].set_ylabel('Detection Rate (%)', fontweight='bold')\n",
    "axes[1].set_title('Anomaly Detection Rate')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Comparison visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c500c",
   "metadata": {},
   "source": [
    "## FINAL RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 3 FINAL SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3 SUCCESSFULLY COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéâ MODELS TRAINED AND EVALUATED:\n",
    "\n",
    "‚úÖ Isolation Forest - 3 different train/test splits\n",
    "‚úÖ Local Outlier Factor (LOF) - 3 different train/test splits\n",
    "‚úÖ One-Class SVM - 3 different train/test splits\n",
    "‚úÖ Autoencoder (Deep Learning) - 3 different train/test splits\n",
    "\n",
    "üìä TRAIN/TEST SPLITS EVALUATED:\n",
    "   ‚Ä¢ 70% Training / 30% Testing\n",
    "   ‚Ä¢ 60% Training / 40% Testing\n",
    "   ‚Ä¢ 80% Training / 20% Testing\n",
    "\n",
    "üîç TOTAL STUDENTS ANALYZED:\n",
    "   ‚Ä¢ Isolation Forest anomalies: {comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Anomalies'].sum()}\n",
    "   ‚Ä¢ LOF anomalies: {comparison_df[comparison_df['Model']=='LOF']['Test_Anomalies'].sum()}\n",
    "   ‚Ä¢ One-Class SVM anomalies: {comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Anomalies'].sum()}\n",
    "   ‚Ä¢ Autoencoder anomalies: {comparison_df[comparison_df['Model']=='Autoencoder']['Test_Anomalies'].sum()}\n",
    "\n",
    "üìà DETECTION RATES (Average across all splits):\n",
    "   ‚Ä¢ Isolation Forest: {comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].mean():.2f}%\n",
    "   ‚Ä¢ LOF: {comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].mean():.2f}%\n",
    "   ‚Ä¢ One-Class SVM: {comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].mean():.2f}%\n",
    "   ‚Ä¢ Autoencoder: {comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].mean():.2f}%\n",
    "\n",
    "üèÜ BEST PERFORMING CONFIGURATIONS:\n",
    "   ‚Ä¢ Isolation Forest: {comparison_df[comparison_df['Model']=='Isolation Forest'].loc[comparison_df[comparison_df['Model']=='Isolation Forest']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "   ‚Ä¢ LOF: {comparison_df[comparison_df['Model']=='LOF'].loc[comparison_df[comparison_df['Model']=='LOF']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "   ‚Ä¢ One-Class SVM: {comparison_df[comparison_df['Model']=='One-Class SVM'].loc[comparison_df[comparison_df['Model']=='One-Class SVM']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "   ‚Ä¢ Autoencoder: {comparison_df[comparison_df['Model']=='Autoencoder'].loc[comparison_df[comparison_df['Model']=='Autoencoder']['Test_Rate_%'].idxmax(), 'Split']} split\n",
    "\n",
    "‚ú® Next Phase: Phase 4 - VALIDATION (Consensus and Analysis)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Ready for Phase 4: VALIDATION AND CONSENSUS ANALYSIS\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
